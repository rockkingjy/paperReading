## Papers 

* 1802.Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples [[Code](https://github.com/anishathalye/obfuscated-gradients)]

* 1711.Towards Deep Learning Models Resistant to Adversarial Attacks [[paper](https://arxiv.org/pdf/1706.06083.pdf)]

* 1704.Security and Privacy Weaknesses of Neural Networks [[paper](https://matt.life/papers/security_privacy_neural_networks.pdf)]

* 1700.Towards Evaluating the Robustness of Neural Networks [[code](https://github.com/carlini/nn_robust_attacks)]

* 1702.openAI.Attacking Machine Learning with Adversarial Examples [[blog](https://blog.openai.com/adversarial-example-research/)]

* 1701.Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks [[paper](https://arxiv.org/pdf/1701.04143.pdf)]

* 1611.Towards the Science of Security and Privacy in Machine Learning [[paper](https://arxiv.org/abs/1611.03814)]

* 1610.cleverhans v2.0.0: an adversarial machine learning library [[paper](https://arxiv.org/pdf/1610.00768.pdf)] [[code](https://github.com/tensorflow/cleverhans)]

* 1607.Adversarial examples in the physical world [[paper](https://arxiv.org/pdf/1607.02533.pdf)]

* 1602.Practical Black-Box Attacks against Machine Learning [[paper](https://arxiv.org/pdf/1602.02697.pdf)]: Gradient masking.

* 1511.Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks [[paper](https://arxiv.org/pdf/1511.04508.pdf)]

* 1412.Explaining and Harnessing Adversarial Examples [[paper](https://arxiv.org/pdf/1412.6572.pdf)]

* 1312.Intriguing properties of neural networks [[paper](https://arxiv.org/pdf/1312.6199.pdf)]: Adversarial examples.

[[paper]()]

## Concepts

Andversarial examples: Inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake.

Gradient masking: trying to deny the attacker access to a useful gradient.
