## 

* 1802.Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples[[Code](https://github.com/anishathalye/obfuscated-gradients)]

* 1711.Towards Deep Learning Models Resistant to Adversarial Attacks[[paper](https://arxiv.org/pdf/1706.06083.pdf)]

* 1702.openAI.Attacking Machine Learning with Adversarial Examples[[Blog](https://blog.openai.com/adversarial-example-research/)]

* 1402.Intriguing properties of neural networks [[paper](https://arxiv.org/pdf/1312.6199.pdf)]
