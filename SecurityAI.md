## 

* 1802.Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples[[Code](https://github.com/anishathalye/obfuscated-gradients)]

* 1711.Towards Deep Learning Models Resistant to Adversarial Attacks[[paper](https://arxiv.org/pdf/1706.06083.pdf)]

* Practical Black-Box Attacks against Machine Learning [[paper](https://arxiv.org/pdf/1602.02697.pdf)]

* 1700.Towards Evaluating the Robustness of Neural Networks [[Code](https://github.com/carlini/nn_robust_attacks)]

* 1702.openAI.Attacking Machine Learning with Adversarial Examples[[Blog](https://blog.openai.com/adversarial-example-research/)]

* 1402.Intriguing properties of neural networks [[paper](https://arxiv.org/pdf/1312.6199.pdf)]
